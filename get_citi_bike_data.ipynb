{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needed imports\n",
    "Importing all the packages we need for the data we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# doing certain tasks in parallel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# for general os operations\n",
    "import os\n",
    "\n",
    "# for unzipping zip files\n",
    "import zipfile\n",
    "\n",
    "# for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# for database storage\n",
    "import sqlite3\n",
    "\n",
    "# for json work\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 227 total files.\n"
     ]
    }
   ],
   "source": [
    "# below is the base url where all the data lives\n",
    "url = 'https://s3.amazonaws.com/tripdata/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'xml')\n",
    "\n",
    "all_data_files = soup.find_all('Key')\n",
    "\n",
    "print(f'There are {len(all_data_files)} total files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to save csv files\n",
    "csv_save_path = 'data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SQLite database at 'data/database/citibike_data.db' exists and the max start_at is 2023-11-30 23:59:57\n"
     ]
    }
   ],
   "source": [
    "# Let's check to see if the database exists. If so, let's find the latest bit of data and ensure that we're not redownloading processed data\n",
    "database_path = 'data/database/citibike_data.db'\n",
    "\n",
    "database_exists = False\n",
    "\n",
    "if os.path.exists(database_path):\n",
    "    database_exists = True\n",
    "    \n",
    "    with sqlite3.connect(database_path) as connection:\n",
    "        \n",
    "        # Create a cursor object\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Execute the SQL query to get the maximum date table\n",
    "        max_year_query = \"\"\"\n",
    "        SELECT \n",
    "            max(cast(yr as INTEGER)) as max_yr\n",
    "        FROM (\n",
    "            SELECT SUBSTR(name, -4) as yr \n",
    "            FROM sqlite_master \n",
    "            WHERE \n",
    "                type='table'            \n",
    "        )\n",
    "        ;\n",
    "        \"\"\"\n",
    "        cursor.execute(max_year_query)\n",
    "\n",
    "        # Fetch the result\n",
    "        max_year_table = cursor.fetchone()[0]\n",
    "\n",
    "        # Execute the query to get the max date\n",
    "        max_date_query = f\"\"\"\n",
    "        select max(started_at) as max_started_at\n",
    "        from citibike_rides_{max_year_table}\n",
    "        \"\"\"\n",
    "        cursor.execute(max_date_query)\n",
    "\n",
    "        # Fetch the result\n",
    "        max_database_date = datetime.strptime(cursor.fetchone()[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        \n",
    "    print(f\"The SQLite database at '{database_path}' exists and the max start_at is {max_database_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 file(s) to download.\n"
     ]
    }
   ],
   "source": [
    "# Create a function to download files\n",
    "def download_and_unzip_file(url, save_path):\n",
    "    \n",
    "    # Download the file\n",
    "    r = requests.get(url)\n",
    "    with open(save_path, 'wb') as file:\n",
    "        file.write(r.content)\n",
    "\n",
    "    # Extract the CSV from the downloaded zip file\n",
    "    with zipfile.ZipFile(save_path, 'r') as zipped_file:\n",
    "        csv_filename = [name for name in zipped_file.namelist()]\n",
    "        zipped_file.extract(csv_filename[0], os.path.dirname(save_path))\n",
    "    \n",
    "    # Delete the zip file\n",
    "    os.remove(save_path)\n",
    "\n",
    "    print(f'\\tCompleted {csv_filename[0]}')\n",
    "    \n",
    "\n",
    "# Create a function that'll download each file in parallel.\n",
    "#   This takes a list of file URLs and a common save directory.\n",
    "def download_files_in_parallel(file_urls, save_dir):\n",
    "    total_system_workers = os.cpu_count() or 1\n",
    "    usable_workers = max(1, total_system_workers)\n",
    "    with ThreadPoolExecutor(max_workers=usable_workers) as executor:\n",
    "        # Use executor.map to apply the download_file function to each URL\n",
    "        # Use save_dir as the common save directory for all files\n",
    "        executor.map(lambda url: download_and_unzip_file(url, os.path.join(save_dir, os.path.basename(url))), file_urls)\n",
    "\n",
    "# Initialize a list of the files we'll want to download.\n",
    "files_to_download = []\n",
    "\n",
    "# Here we'll only grab NYC rides. We'll do this by first checking that the file is a .zip file and then ensuring that it doesn't contain 'JC' in the title.\n",
    "for each_file in all_data_files:\n",
    "    temp_file_name  = each_file.get_text()\n",
    "    temp_filetype   = temp_file_name[-4:]\n",
    "    \n",
    "    if temp_filetype == '.zip' and not('JC' in temp_file_name):\n",
    "        \n",
    "        # We'll also ensure that the file contains new data by comparing the months only if the database exists.\n",
    "        if database_exists:\n",
    "            if datetime.strptime(temp_file_name[:6], '%Y%m') >= max_database_date:\n",
    "                files_to_download.append(url + temp_file_name)\n",
    "        else:\n",
    "            files_to_download.append(url + temp_file_name)\n",
    "\n",
    "print(f'{len(files_to_download)} file(s) to download.')\n",
    "\n",
    "download_files_in_parallel(\n",
    "    files_to_download,\n",
    "    csv_save_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this data, we need to make sure that column names align. \n",
    "# We're going to have everything align to the most recent data file (202311-citibike-tripdata.csv)\n",
    "if not database_exists:\n",
    "    # This funciton will check if column names align with the expected list\n",
    "    def check_column_alignment(test_df, expected_columns):\n",
    "        actual_columns = test_df.columns.tolist()\n",
    "        return set(actual_columns) == set(expected_columns)\n",
    "\n",
    "    # Read in the ideal data archetecture\n",
    "    ideal_df = pd.read_csv(\n",
    "        csv_save_path + '/202311-citibike-tripdata.csv',\n",
    "        nrows=100\n",
    "    )\n",
    "\n",
    "    # create a list of ideal columns\n",
    "    ideal_columns = ideal_df.columns.tolist()\n",
    "\n",
    "    ideal_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not database_exists:\n",
    "    # Initialize a list of non-ideal columns\n",
    "    non_ideal_columnsets = []\n",
    "\n",
    "    # Iterate through each file and document those that aren't ideal\n",
    "    for file_name in os.listdir(csv_save_path):\n",
    "        temp_df = pd.read_csv(\n",
    "            os.path.join(csv_save_path,file_name),\n",
    "            nrows=100\n",
    "        )\n",
    "        if not(check_column_alignment(temp_df, ideal_columns)):\n",
    "            non_ideal_columnsets.append(set(temp_df.columns.tolist()))\n",
    "\n",
    "    # Get the unique set of non_ideal_columnssets\n",
    "    non_ideal_unique = list(map(set,set(map(frozenset, non_ideal_columnsets))))\n",
    "\n",
    "    # show the unique datasets that need to be mapped\n",
    "    non_ideal_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remind me what the ideal columns are:\n",
    "if not database_exists:\n",
    "    ideal_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if database_exists:\n",
    "    with open('config/column_map.json', 'r') as json_file:\n",
    "        column_mapping = json.load(json_file)\n",
    "else:\n",
    "    # Initialize an empty dictionary\n",
    "    column_mapping = {}\n",
    "\n",
    "    # Ask the user how each item should be mapped\n",
    "    for each_set in non_ideal_unique:\n",
    "        for each_column in each_set:\n",
    "            column_mapping[each_column] = input(f'What goes in for \"{each_column}\"?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end station longitude': 'end_lng',\n",
       " 'stoptime': 'ended_at',\n",
       " 'start station id': 'start_station_id',\n",
       " 'end station latitude': 'end_lat',\n",
       " 'start station name': 'start_station_name',\n",
       " 'tripduration': 'trip_duration',\n",
       " 'gender': 'gender',\n",
       " 'start station longitude': 'start_lng',\n",
       " 'starttime': 'started_at',\n",
       " 'end station id': 'end_station_id',\n",
       " 'end station name': 'end_station_name',\n",
       " 'bikeid': 'bike_id',\n",
       " 'start station latitude': 'start_lat',\n",
       " 'usertype': 'member_casual',\n",
       " 'birth year': 'birth_year',\n",
       " 'Start Station Name': 'start_station_name',\n",
       " 'End Station Latitude': 'end_lat',\n",
       " 'Start Station Latitude': 'start_lat',\n",
       " 'Bike ID': 'bike_id',\n",
       " 'Birth Year': 'birth_year',\n",
       " 'End Station Name': 'end_station_name',\n",
       " 'End Station ID': 'end_station_id',\n",
       " 'Start Time': 'started_at',\n",
       " 'User Type': 'member_casual',\n",
       " 'Gender': 'gender',\n",
       " 'Start Station Longitude': 'start_lng',\n",
       " 'End Station Longitude': 'end_lng',\n",
       " 'Start Station ID': 'start_station_id',\n",
       " 'Trip Duration': 'trip_duration',\n",
       " 'Stop Time': 'ended_at'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save column mapping to a json in the config file\n",
    "with open('config/column_map.json', 'w') as json_file:\n",
    "    json.dump(column_mapping, json_file, indent=2)\n",
    "\n",
    "column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that every column mapped properly\n",
    "\n",
    "if len(os.listdir(csv_save_path)) > 0:\n",
    "    sample_data = []\n",
    "\n",
    "    for file_name in os.listdir(csv_save_path):\n",
    "        temp_df = pd.read_csv(\n",
    "            os.path.join(csv_save_path,file_name),\n",
    "            nrows=100,\n",
    "            parse_dates=True\n",
    "        )\n",
    "\n",
    "        temp_df.rename(\n",
    "            columns = column_mapping,\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        temp_df['started_at'] = pd.to_datetime(temp_df['started_at'])\n",
    "        temp_df['ended_at'] = pd.to_datetime(temp_df['ended_at'])\n",
    "\n",
    "        sample_data.append(\n",
    "            temp_df\n",
    "        )\n",
    "\n",
    "    sample_df = pd.concat(sample_data)\n",
    "    sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database (creates a new database if it doesn't exist)\n",
    "if len(os.listdir(csv_save_path)) > 0:\n",
    "    with sqlite3.connect(database_path) as connection:\n",
    "        \n",
    "        for file_name in os.listdir(csv_save_path):\n",
    "            temp_save_table = f'citibike_rides_{file_name[:4]}'\n",
    "            print(f'Writing \"{file_name}\" to \"{temp_save_table}\"...')\n",
    "            temp_df = pd.read_csv(\n",
    "                os.path.join(csv_save_path,file_name),\n",
    "                header=0,\n",
    "                low_memory=False\n",
    "            )\n",
    "\n",
    "            temp_df.rename(\n",
    "                columns = column_mapping,\n",
    "                inplace=True\n",
    "            )\n",
    "\n",
    "            t_col_list = list(temp_df.columns)\n",
    "\n",
    "            # Set datatypes.\n",
    "\n",
    "            temp_df['started_at'] = pd.to_datetime(temp_df['started_at'])\n",
    "            temp_df['ended_at'] = pd.to_datetime(temp_df['ended_at'])\n",
    "\n",
    "            # If a column doesn't exist, then create it with null data and the datatype from sample_df\n",
    "            for each_column in list(sample_df.columns):\n",
    "                if each_column not in list(temp_df.columns):\n",
    "                    temp_df[each_column] = np.nan\n",
    "                    temp_df[each_column] = temp_df[each_column].astype(sample_df.dtypes[each_column])\n",
    "\n",
    "            temp_df.to_sql(\n",
    "                temp_save_table,\n",
    "                connection,\n",
    "                index=False,\n",
    "                if_exists='append'\n",
    "            )\n",
    "\n",
    "            \n",
    "            connection.commit()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(os.listdir(csv_save_path)) > 0:\n",
    "    for each_file in os.listdir(csv_save_path):\n",
    "        file_path = os.path.join(csv_save_path, each_file)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>member_casual</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>634</td>\n",
       "      <td>2013-07-01 00:00:00</td>\n",
       "      <td>2013-07-01 00:10:34</td>\n",
       "      <td>164</td>\n",
       "      <td>E 47 St &amp; 2 Ave</td>\n",
       "      <td>40.753231</td>\n",
       "      <td>-73.970325</td>\n",
       "      <td>504</td>\n",
       "      <td>1 Ave &amp; E 15 St</td>\n",
       "      <td>40.732219</td>\n",
       "      <td>-73.981656</td>\n",
       "      <td>16950</td>\n",
       "      <td>Customer</td>\n",
       "      <td>\\N</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1547</td>\n",
       "      <td>2013-07-01 00:00:02</td>\n",
       "      <td>2013-07-01 00:25:49</td>\n",
       "      <td>388</td>\n",
       "      <td>W 26 St &amp; 10 Ave</td>\n",
       "      <td>40.749718</td>\n",
       "      <td>-74.002950</td>\n",
       "      <td>459</td>\n",
       "      <td>W 20 St &amp; 11 Ave</td>\n",
       "      <td>40.746745</td>\n",
       "      <td>-74.007756</td>\n",
       "      <td>19816</td>\n",
       "      <td>Customer</td>\n",
       "      <td>\\N</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>178</td>\n",
       "      <td>2013-07-01 00:01:04</td>\n",
       "      <td>2013-07-01 00:04:02</td>\n",
       "      <td>293</td>\n",
       "      <td>Lafayette St &amp; E 8 St</td>\n",
       "      <td>40.730287</td>\n",
       "      <td>-73.990765</td>\n",
       "      <td>237</td>\n",
       "      <td>E 11 St &amp; 2 Ave</td>\n",
       "      <td>40.730473</td>\n",
       "      <td>-73.986724</td>\n",
       "      <td>14548</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1980</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1580</td>\n",
       "      <td>2013-07-01 00:01:06</td>\n",
       "      <td>2013-07-01 00:27:26</td>\n",
       "      <td>531</td>\n",
       "      <td>Forsyth St &amp; Broome St</td>\n",
       "      <td>40.718939</td>\n",
       "      <td>-73.992663</td>\n",
       "      <td>499</td>\n",
       "      <td>Broadway &amp; W 60 St</td>\n",
       "      <td>40.769155</td>\n",
       "      <td>-73.981918</td>\n",
       "      <td>16063</td>\n",
       "      <td>Customer</td>\n",
       "      <td>\\N</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>757</td>\n",
       "      <td>2013-07-01 00:01:10</td>\n",
       "      <td>2013-07-01 00:13:47</td>\n",
       "      <td>382</td>\n",
       "      <td>University Pl &amp; E 14 St</td>\n",
       "      <td>40.734927</td>\n",
       "      <td>-73.992005</td>\n",
       "      <td>410</td>\n",
       "      <td>Suffolk St &amp; Stanton St</td>\n",
       "      <td>40.720664</td>\n",
       "      <td>-73.985180</td>\n",
       "      <td>19213</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_duration           started_at             ended_at  start_station_id  \\\n",
       "0            634  2013-07-01 00:00:00  2013-07-01 00:10:34               164   \n",
       "1           1547  2013-07-01 00:00:02  2013-07-01 00:25:49               388   \n",
       "2            178  2013-07-01 00:01:04  2013-07-01 00:04:02               293   \n",
       "3           1580  2013-07-01 00:01:06  2013-07-01 00:27:26               531   \n",
       "4            757  2013-07-01 00:01:10  2013-07-01 00:13:47               382   \n",
       "\n",
       "        start_station_name  start_lat  start_lng  end_station_id  \\\n",
       "0          E 47 St & 2 Ave  40.753231 -73.970325             504   \n",
       "1         W 26 St & 10 Ave  40.749718 -74.002950             459   \n",
       "2    Lafayette St & E 8 St  40.730287 -73.990765             237   \n",
       "3   Forsyth St & Broome St  40.718939 -73.992663             499   \n",
       "4  University Pl & E 14 St  40.734927 -73.992005             410   \n",
       "\n",
       "          end_station_name    end_lat    end_lng  bike_id member_casual  \\\n",
       "0          1 Ave & E 15 St  40.732219 -73.981656    16950      Customer   \n",
       "1         W 20 St & 11 Ave  40.746745 -74.007756    19816      Customer   \n",
       "2          E 11 St & 2 Ave  40.730473 -73.986724    14548    Subscriber   \n",
       "3       Broadway & W 60 St  40.769155 -73.981918    16063      Customer   \n",
       "4  Suffolk St & Stanton St  40.720664 -73.985180    19213    Subscriber   \n",
       "\n",
       "  birth_year  gender ride_id rideable_type  \n",
       "0         \\N       0    None          None  \n",
       "1         \\N       0    None          None  \n",
       "2       1980       2    None          None  \n",
       "3         \\N       0    None          None  \n",
       "4       1986       1    None          None  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at the 2013 data\n",
    "with sqlite3.connect(database_path) as connection:\n",
    "    df = pd.read_sql_query(\n",
    "        'select * from citibike_rides_2013 limit 100',\n",
    "        connection\n",
    "    )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55262E4365A955A2</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2022-01-18 08:23:52</td>\n",
       "      <td>2022-01-18 08:28:18</td>\n",
       "      <td>Boerum Pl\\t&amp; Pacific St</td>\n",
       "      <td>4488.09</td>\n",
       "      <td>Clinton St &amp; Joralemon St</td>\n",
       "      <td>4605.04</td>\n",
       "      <td>40.688489</td>\n",
       "      <td>-73.991160</td>\n",
       "      <td>40.692395</td>\n",
       "      <td>-73.993379</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D272F1B15D841EC0</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2022-01-21 09:03:22</td>\n",
       "      <td>2022-01-21 09:05:44</td>\n",
       "      <td>E 12 St &amp; Ave C</td>\n",
       "      <td>5616.08</td>\n",
       "      <td>E 10 St &amp; Avenue A</td>\n",
       "      <td>5659.05</td>\n",
       "      <td>40.727243</td>\n",
       "      <td>-73.976831</td>\n",
       "      <td>40.727408</td>\n",
       "      <td>-73.981420</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1FCEF55EB4A807F</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2022-01-22 14:28:32</td>\n",
       "      <td>2022-01-22 14:53:18</td>\n",
       "      <td>W 21 St &amp; 6 Ave</td>\n",
       "      <td>6140.05</td>\n",
       "      <td>W 44 St &amp; 11 Ave</td>\n",
       "      <td>6756.05</td>\n",
       "      <td>40.741740</td>\n",
       "      <td>-73.994156</td>\n",
       "      <td>40.762009</td>\n",
       "      <td>-73.996975</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E9CBDC6A0162C068</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2022-01-19 14:49:47</td>\n",
       "      <td>2022-01-19 14:54:02</td>\n",
       "      <td>38 St &amp; 30 Ave</td>\n",
       "      <td>6850.01</td>\n",
       "      <td>Crescent St &amp; 30 Ave</td>\n",
       "      <td>6958.06</td>\n",
       "      <td>40.764175</td>\n",
       "      <td>-73.915840</td>\n",
       "      <td>40.768692</td>\n",
       "      <td>-73.924957</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2177A5B57326CE9B</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2022-01-16 14:36:06</td>\n",
       "      <td>2022-01-16 14:44:06</td>\n",
       "      <td>Pacific St &amp; Nevins St</td>\n",
       "      <td>4362.04</td>\n",
       "      <td>Clinton St &amp; Tillary St</td>\n",
       "      <td>4748.07</td>\n",
       "      <td>40.685376</td>\n",
       "      <td>-73.983021</td>\n",
       "      <td>40.696233</td>\n",
       "      <td>-73.991421</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type           started_at             ended_at  \\\n",
       "0  55262E4365A955A2   classic_bike  2022-01-18 08:23:52  2022-01-18 08:28:18   \n",
       "1  D272F1B15D841EC0   classic_bike  2022-01-21 09:03:22  2022-01-21 09:05:44   \n",
       "2  D1FCEF55EB4A807F   classic_bike  2022-01-22 14:28:32  2022-01-22 14:53:18   \n",
       "3  E9CBDC6A0162C068  electric_bike  2022-01-19 14:49:47  2022-01-19 14:54:02   \n",
       "4  2177A5B57326CE9B  electric_bike  2022-01-16 14:36:06  2022-01-16 14:44:06   \n",
       "\n",
       "        start_station_name start_station_id           end_station_name  \\\n",
       "0  Boerum Pl\\t& Pacific St          4488.09  Clinton St & Joralemon St   \n",
       "1          E 12 St & Ave C          5616.08         E 10 St & Avenue A   \n",
       "2          W 21 St & 6 Ave          6140.05           W 44 St & 11 Ave   \n",
       "3           38 St & 30 Ave          6850.01       Crescent St & 30 Ave   \n",
       "4   Pacific St & Nevins St          4362.04    Clinton St & Tillary St   \n",
       "\n",
       "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \\\n",
       "0        4605.04  40.688489 -73.991160  40.692395 -73.993379        member   \n",
       "1        5659.05  40.727243 -73.976831  40.727408 -73.981420        member   \n",
       "2        6756.05  40.741740 -73.994156  40.762009 -73.996975        member   \n",
       "3        6958.06  40.764175 -73.915840  40.768692 -73.924957        member   \n",
       "4        4748.07  40.685376 -73.983021  40.696233 -73.991421        member   \n",
       "\n",
       "  trip_duration bike_id birth_year gender  \n",
       "0          None    None       None   None  \n",
       "1          None    None       None   None  \n",
       "2          None    None       None   None  \n",
       "3          None    None       None   None  \n",
       "4          None    None       None   None  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at the 2022 data\n",
    "with sqlite3.connect(database_path) as connection:\n",
    "    df = pd.read_sql_query(\n",
    "        'select * from citibike_rides_2022 limit 100',\n",
    "        connection\n",
    "    )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
